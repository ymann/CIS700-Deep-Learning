{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymann/CIS700-Deep-Learning/blob/master/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GK7XEDAs-aoN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone Git (0 pts)"
      ]
    },
    {
      "metadata": {
        "id": "X0PN1qeC-Qu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the following to get the required files needed for this assignment. "
      ]
    },
    {
      "metadata": {
        "id": "6C1SLvlD7tzu",
        "colab_type": "code",
        "outputId": "1058a903-5c08-47a7-d092-a1e8ce16d5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cis700/hw1-release.git\n",
        "!mv hw1-release/dills/* .\n",
        "!mv hw1-release hw1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw1-release'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 26 (delta 6), reused 23 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sin2bVyRSdP4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classical vs. Deep Learning Approach\n"
      ]
    },
    {
      "metadata": {
        "id": "55fvftr2QDm-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 1. Using Logistic Regression to Classify Fashion-MNIST (10 pts)"
      ]
    },
    {
      "metadata": {
        "id": "58yJGsmVSkMG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Understanding the Dataset**\n",
        "\n",
        "The first step to machine learning is to understand the dataset that you're given. Therefore, familiarize yourself with the Fashion-MNIST dataset. We recommend reading the paper published about the dataset, which is detailed [here](https://arxiv.org/pdf/1708.07747.pdf).\n",
        "\n",
        "**Q1a (2 pts):** Explain at a high level what the dataset is (i.e. what are the specifications of the input data and associated labels)."
      ]
    },
    {
      "metadata": {
        "id": "1jGrylNPlHxM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading the Data**\n",
        "\n",
        "Since there's no built in dataloader for Fashion-MNIST built into sci-kit learn, we've created a dataloader that you may use. The first code snippet is to the install [wget](https://pypi.org/project/wget/) and sets up the directory structure for your data to go in.\n",
        "\n",
        "In the second code snippet, we provide the function made by the authors of the fashion-MNIST dataset to extract the downloaded data.\n",
        "\n",
        "**Q1b (3 pts):** Fill in the load_datasets function such that it returns the training data, training labels, testing data, and testing labels. In your report, indicate the dimensions of the training set and the testing set respectively. "
      ]
    },
    {
      "metadata": {
        "id": "wawpPWtf0dtu",
        "colab_type": "code",
        "outputId": "40555762-9951-4d8b-a9e2-90e971f7bb4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!mkdir -p data/fashion"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mOBZr3rLTW46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import wget\n",
        "import pdb\n",
        "\n",
        "## Function provided by the creators of Fashion-MNIST to unpack and load their data\n",
        "def load_mnist(path, kind='train'):\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MLQSFsdUfNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fill in this method to load the Fashion-MNIST dataset\n",
        "def load_datasets():\n",
        "  base_URL = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
        "  files = [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\", \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]\n",
        "  fashion_dir = \"data/fashion\"\n",
        "  wget.download(\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\" + files[0], fashion_dir)\n",
        "  wget.download(\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\" + files[1], fashion_dir)\n",
        "  wget.download(\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\" + files[2], fashion_dir)\n",
        "  wget.download(\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\" + files[3], fashion_dir)\n",
        "\n",
        "  train_images, train_labels = load_mnist(fashion_dir, 'train')\n",
        "  test_images, test_labels = load_mnist(fashion_dir, 't10k')\n",
        "  # Return the correct four variables\n",
        "  return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "train_images, train_labels, test_images, test_labels = load_datasets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L3V7Ts2Lcl_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a6a206d-8f9e-4772-8d0c-5c4a64a437a1"
      },
      "cell_type": "code",
      "source": [
        "print(len(train_images), len(train_images[0]))\n",
        "print(len(train_labels))\n",
        "print(len(test_images), len(test_images[0]))\n",
        "print(len(test_labels))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 784\n",
            "60000\n",
            "10000 784\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AlnqKnqBmCiB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating a Classical Classifier**\n",
        "\n",
        "**Q1c (5 pts):** Use logistic regression to classify the Fashion-MNIST dataset. Note that because this is not sped up by a GPU this may take a few minutes to run. In your report, plot the learning curve for train and validation accuracy and report the final classification accuracy.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fKhoD8PQ9fAV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from mpl_toolkits.mplot3d import Axes3D\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EjnEdexcfzb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2135
        },
        "outputId": "49324089-7a20-4efa-c83a-52a7ecb36c4a"
      },
      "cell_type": "code",
      "source": [
        "# best_c = 0\n",
        "# max_score = 0\n",
        "\n",
        "# for c in range(-4,4):\n",
        "#   if c==0:\n",
        "#     continue\n",
        "#   model = LogisticRegression(solver='lbfgs', multi_class='auto', random_state=0, penalty=\"l2\", C=(10**(c)))\n",
        "#   model.fit(train_images,train_labels)\n",
        "#   score = model.score(test_images,test_labels)\n",
        "#   best_c = c if score > max_score else best_c\n",
        "#   max_score = score if score > max_score else max_score\n",
        "# print(max_score)\n",
        "cv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n",
        "model = LogisticRegression(solver='lbfgs', multi_class='auto', random_state=0, penalty=\"l2\", C=(10**(0.842)))\n",
        "model.fit(train_images,train_labels)\n",
        "train_sizes, train_scores, test_scores = learning_curve(model, train_images,train_labels, cv=cv)\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "print(model.score(test_images,test_labels))\n",
        "print(model.score(train_images,train_labels))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8413\n",
            "0.8627333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFKCAYAAAAJyrb2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "dmsoPT5Cmo7E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 2. Using Deep Learning to Classify Fashion-MNIST (10 pts)\n",
        "\n",
        "In this section you will also classify the Fashion-MNIST dataset, however, with a deep neural network instead.\n",
        "\n",
        "**Downloading PyTorch**\n",
        "\n",
        "Run the code snippet below to install PyTorch / required packages and set up the GPU."
      ]
    },
    {
      "metadata": {
        "id": "R7YYE8Wm7aml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Required packages\n",
        "!pip install tensorflow\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install Pillow\n",
        "!pip install image\n",
        "\n",
        "## Add any other packages you may need below"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2EVPAWLR3W3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "  \n",
        "import torch\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M5dETVerGj_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**PyTorch Tensorboard Integration**\n",
        "\n",
        "In this class we present the option of using tensorboard to visualize loss / accuracy plots and images. You may choose to use matplotlib instead, however, we recommend using tensorboard as it's becoming the industry standard for analyzing / debugging neural networks. \n",
        "\n",
        "Simply run the code snippets below in the order provided, and click on the link to open up the tensorboard dashboard (which should be blank right now!)."
      ]
    },
    {
      "metadata": {
        "id": "7oiaJ44FDxmp",
        "colab_type": "code",
        "outputId": "91883576-f862-4939-f97f-b18a407a039c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ngrok already installed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZoJmRjNmF8HL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c842voLNGJdY",
        "colab_type": "code",
        "outputId": "16069240-70c7-4dac-9018-1a6bf8955381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorboard Link: http://36e68836.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-G5XfE0HQFFx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To see how to use Pytorch with tensorboard, check out [this](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/04-utils/tensorboard/main.py) github repo (specifically in main.py). Try plotting something using tensorboard to ensure that it's all working. For help, come to office hours or check out [this](https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab) link (where we got the tensorboard-colab integration).\n",
        "\n",
        "**Important:** Note that we've changed the logging folder to \"./logs\" . Keep that in mind when using tensorboard."
      ]
    },
    {
      "metadata": {
        "id": "2btBOJ_gpmdq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**PyTorch Dataloaders **\n",
        "\n",
        "The first step to deep learning is to create a dataloader. It's important to understand how a dataloader works so you can create a custom dataloader for datasets in the future. To understand more, we highly recommend PyTorch's tutorial on dataloaders detailed [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "**Q2a (2 pts):** Create a training and testing dataloader for the Fashion-MNIST dataset. Report the size of the training and testing sets respectively (it should match those from the previous question.)\n",
        "\n",
        "*Hint:* Fashion-MNIST is a highly used dataset, therefore you do not have to create a whole dataloader from scratch. For some help, take a look at Torch's [dataset](https://pytorch.org/docs/stable/torchvision/datasets.html) and [dataloader](https://pytorch.org/docs/stable/data.html) page."
      ]
    },
    {
      "metadata": {
        "id": "JwU2URVvTy_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision as tp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y0eMLjAj-wp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = tp.datasets.FashionMNIST(root=\"./data/fashion\",train=True, download=True, transform=transforms.ToTensor())\n",
        "test = tp.datasets.FashionMNIST(root=\"./data/fashion\",train=False, download=True, transform=transforms.ToTensor(),)\n",
        "train_loader = torch.utils.data.DataLoader(batch_size=100, shuffle=True, dataset=train)\n",
        "test_loader = torch.utils.data.DataLoader(batch_size=100, shuffle=True, dataset=test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypzIoleDB6nr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2dbcb8b-565e-4dcb-d7ed-f3c92df43ba7"
      },
      "cell_type": "code",
      "source": [
        "print(len(test))\n",
        "print(len(train))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mUPsp_iYp6HB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating the Neural Network Model**\n",
        "\n",
        "PyTorch has a fantastic tutorial on creating a neural network model, detailed [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n",
        "\n",
        "**Q2b (2 pts):** Using knowledge from lecture and the tutorial from the link above, create a neural network model to classify the Fashion-MNIST dataset. In your report, detail your network architecture, explaining the choices that you made (i.e. depth of the neural network, activation functions, etc).\n",
        "\n",
        "*NOTE:* You may **not** use convolutional layers in this model."
      ]
    },
    {
      "metadata": {
        "id": "V2GEI9xWUk0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pcO78KpZqnc9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating the Training Loop**\n",
        "\n",
        "**Q2c (2 pts):** Create a training loop that goes over the entire Fashion-MNIST dataset once. You should tune all hyperparameters (i.e. batch-size, learning rate, etc). Use tensorboard so that in one graph, plot accuracy vs iterations, and in the other plot loss vs iterations (axis labels are not needed if you're using tensorboard). Report the final testing accuracy and loss."
      ]
    },
    {
      "metadata": {
        "id": "BS1RH2Uhqn3K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use this object to help with tensorboard integration!\n",
        "from hw1.helper import Logger\n",
        "logger = Logger('./logs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CIXTJze5BU2G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q2d (2 pts):** Create a testing loop that outputs the accuracy of your trained neural network on the test set. Report the evaluation accuracy of your network on this testing set.\n",
        "\n",
        "*Hint:* Look at the torch.no_grad() function to evaluate your neural network without updating the weights."
      ]
    },
    {
      "metadata": {
        "id": "aRBDCwCwsZn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32ljCpPZVISV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q2e (2 pts):** Compare the testing accuracy of your neural network vs. the decision tree clasifier you made in the previous part."
      ]
    },
    {
      "metadata": {
        "id": "L_UJwNd6e_fy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Question 3. Deep Learning Diagnosis"
      ]
    },
    {
      "metadata": {
        "id": "8anIoyXm_KCm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3.1.a MNIST control group (10 pts)"
      ]
    },
    {
      "metadata": {
        "id": "n1_Tpkid_Lpm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the same PyTorch approach as above, tune the hyperparameters for a neural net on the MNIST dataset.  Plot the learning curve and report its final accuracy in your writeup.  This neural net's performance will serve as the reference point (a \"control group\") for your analysis in parts 3.2, 3.4, and 3.7"
      ]
    },
    {
      "metadata": {
        "id": "gokRcC3S_F9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Tune hyperparameters here\n",
        "num_epochs = 5\n",
        "\n",
        "# Load the MNIST dataset (images and labels, both train and test) into 2 DataLoaders\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data_mnist', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data_mnist', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      # TODO: construct the submodules\n",
        "      return\n",
        "\n",
        "    def forward(self, x):\n",
        "      # TODO: define the forward pass\n",
        "      return\n",
        "      \n",
        "# TODO: complete the train loop\n",
        "total_step = len(train_dataset)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_dataset):\n",
        "      pass\n",
        "    \n",
        "# TODO: complete the test loop\n",
        "\n",
        "# TODO: plot the learning curve."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_9ODXL2BTuA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3.1.b CIFAR control group (10 pts)\n",
        "\n",
        "Similar to the previous part, create a neural network, training, and testing loop and tune the hyperparameters for the CIFAR-10 dataset. Plot the learning curve and report its final accuracy in your writeup.  This neural net's performance will serve as the reference point (a \"control group\") for your analysis in parts 3.3, 3.5, and 3.6"
      ]
    },
    {
      "metadata": {
        "id": "8ezAlVKNBVit",
        "colab_type": "code",
        "outputId": "8ccffa8b-0fd7-4695-bb7f-f06f4ce20a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1884
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "# Tune hyperparameters here\n",
        "num_epochs = 5\n",
        "\n",
        "# Load the MNIST dataset (images and labels, both train and test) into 2 DataLoaders\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data_cifar', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data_cifar', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      # TODO: construct the submodules\n",
        "      return\n",
        "\n",
        "    def forward(self, x):\n",
        "      # TODO: define the forward pass\n",
        "      return\n",
        "      \n",
        "# TODO: complete the train loop\n",
        "total_step = len(train_dataset)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_dataset):\n",
        "      pass\n",
        "    \n",
        "# TODO: complete the test loop\n",
        "\n",
        "# TODO: plot the learning curve."
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar/cifar-10-python.tar.gz\n",
            "Failed download. Trying https -> http instead. Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_cifar/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-01a7801ade07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                            \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                            \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                            download=True)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m test_dataset = torchvision.datasets.CIFAR10(root='./data_cifar', \n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# extract file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 print('Failed download. Trying https -> http instead.'\n\u001b[1;32m     46\u001b[0m                       ' Downloading ' + url + ' to ' + fpath)\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GvGlIFRfwDKY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each question below, we will provide 3 PyTorch objects: a DataLoader, a neural network model, and an optimizer (you will load them using dill, a serialization library in Python).  For each question, please complete the following:\n",
        "\n",
        "a.   Train the given neural net using the given DataLoader and the given optimizer.  Plot the learning curve of the network and report the final accuracy in your writeup.\n",
        "\n",
        "b.   Identify whether the neural net seems to be training correctly.  If not, debug the issue (e.g. printing parameters, gradients, outputs) and report your methodology in your writeup. In addition, we highly recommend checking out [this](https://docs.google.com/document/d/11R5IiLMjddIWM2csfjFBZ__Adu0sY0zuVhbS3_5lLBQ/edit?usp=sharing) google doc with information about common failure modes for training deep neural networks.\n",
        "\n",
        "c.   If you identify an issue with any of the 3 given objects, replace the buggy object with one of your own making.  Then retrain the neural net, plot the new learning curve, and report the improved accuracy in your writeup.  Note that in each of the following problems, exactly 1 object out of the DataLoader, the neural net, and the optimizer will be buggy.  You should not have to replace more than 1.\n",
        "\n",
        "Run the code snippet below for some necessary packages / setup."
      ]
    },
    {
      "metadata": {
        "id": "GhVgSR4moTFf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import dill\n",
        "import torch.optim as optim\n",
        "import time, datetime\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjQTgAuufGmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc \n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.FileWriter(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "        \n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7DtdXWfhU0Rk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(net, optimizer, data_loader):\n",
        "  criterion = torch.nn.CrossEntropyLoss()  \n",
        "  model = net.to(device)\n",
        "  data_iter = iter(data_loader)\n",
        "\n",
        "  iter_per_epoch = len(data_loader)\n",
        "  now = time.mktime(datetime.datetime.now().timetuple())\n",
        "  \n",
        "#   def init_weights(m):\n",
        "#     if type(m) == nn.Linear:\n",
        "#         torch.nn.init.xavier_uniform(m.weight)\n",
        "#         m.bias.data.fill_(0.01)\n",
        "\n",
        "#   net.apply(init_weights)\n",
        "\n",
        "  \n",
        "  logger = Logger(f'./logs/run_{now}/')\n",
        "  num_epochs = 5\n",
        "  for i in range (num_epochs):\n",
        "     # Start training\n",
        "    for step in range(iter_per_epoch):\n",
        "\n",
        "      # Reset the data_iter\n",
        "      if (step+1) % iter_per_epoch == 0:\n",
        "          data_iter = iter(data_loader)\n",
        "\n",
        "      # Fetch images and labels\n",
        "      images, labels = next(data_iter)\n",
        "      images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "#       print(labels)\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "      \n",
        "      # ================================================================== #\n",
        "      #                        Tensorboard Logging                         #\n",
        "      # ================================================================== #\n",
        "\n",
        "      # 1. Log scalar values (scalar summary)\n",
        "      info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "      for tag, value in info.items():\n",
        "          logger.scalar_summary(tag, value, (i) * iter_per_epoch + step+1)\n",
        "\n",
        "#       # 2. Log values and gradients of the parameters (histogram summary)\n",
        "#       for tag, value in model.named_parameters():\n",
        "#           tag = tag.replace('.', '/')\n",
        "# #           print(tag, value)\n",
        "#           logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
        "#           logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), i * iter_per_epoch + step+1)\n",
        "\n",
        "     \n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PaYUR1yLIxgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(net, optimizer, data_loader):\n",
        "  criterion = torch.nn.CrossEntropyLoss()  \n",
        "  model = net.to(device)\n",
        "  data_iter = iter(data_loader)\n",
        "\n",
        "  iter_per_epoch = len(data_loader)\n",
        "  now = time.mktime(datetime.datetime.now().timetuple())\n",
        "  \n",
        "#   def init_weights(m):\n",
        "#     if type(m) == nn.Linear:\n",
        "#         torch.nn.init.xavier_uniform(m.weight)\n",
        "#         m.bias.data.fill_(0.01)\n",
        "\n",
        "#   net.apply(init_weights)\n",
        "\n",
        "  \n",
        "  num_epochs = 5\n",
        "  for i in range (num_epochs):\n",
        "     # Start training\n",
        "    for step in range(iter_per_epoch):\n",
        "\n",
        "      # Reset the data_iter\n",
        "      if (step+1) % iter_per_epoch == 0:\n",
        "          data_iter = iter(data_loader)\n",
        "\n",
        "      # Fetch images and labels\n",
        "      images, labels = next(data_iter)\n",
        "      images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "#       print(labels)\n",
        "\n",
        "      # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pInKH8sNfKoG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3.2 MNIST diagnosis 1 (10 pts)"
      ]
    },
    {
      "metadata": {
        "id": "3UTDn_Vk-3GB",
        "colab_type": "code",
        "outputId": "f18677b8-d121-466e-8656-e1ea47f6bdf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "q_3_2_data_loader = dill.load( open( \"q3_2_loader.p\", \"rb\" ) )\n",
        "train_loader, test_loader = q_3_2_data_loader()\n",
        "q_3_2_net_loader = dill.load( open( \"q3_2_net.p\", \"rb\" ) )\n",
        "net, optimizer = q_3_2_net_loader()\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "net.apply(init_weights)\n",
        "# model = train_model(net, optimizer, train_loader)\n",
        "# print(len(iter(train_loader)))\n",
        "# countarr = [0,0,0,0,0,0,0,0,0,0]\n",
        "# for i in range (len(train_loader.dataset)):\n",
        "#   countarr[train_loader.dataset[i][1]] = countarr[train_loader.dataset[i][1]] + 1\n",
        "# print(countarr)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V76qV4--fSXg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3.3 CIFAR diagnosis 1 (10 pts)"
      ]
    },
    {
      "metadata": {
        "id": "I5rswHIx1tqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9fd629e4-724f-4552-ddd5-01018b97ac01"
      },
      "cell_type": "code",
      "source": [
        "q_3_3_data_loader = dill.load( open( \"q3_3_loader.p\", \"rb\" ) )\n",
        "train_loader, test_loader = q_3_3_data_loader()\n",
        "q_3_3_net_loader = dill.load( open( \"q3_3_net.p\", \"rb\" ) )\n",
        "net, optimizer = q_3_3_net_loader()\n",
        "new_train_loader = torch.utils.data.DataLoader(batch_size=100, shuffle=True, dataset=train_loader.dataset)\n",
        "# model = train_model(net, optimizer, new_train_loader)\n",
        "# print(net)\n",
        "print(net`)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Q_3_3_net(\n",
            "  (fc1): Linear(in_features=3072, out_features=8192, bias=True)\n",
            "  (fc2): Linear(in_features=8192, out_features=4096, bias=True)\n",
            "  (fc3): Linear(in_features=4096, out_features=2048, bias=True)\n",
            "  (fc4): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RcNBMPnAfT11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3.4 MNIST diagnosis 2 (10 pts)\n",
        "\n",
        "*Hint:* Look at the test accuracy"
      ]
    },
    {
      "metadata": {
        "id": "DeYVw-FQieNm",
        "colab_type": "code",
        "outputId": "84d90939-3216-4940-d9ac-dd7f931e0030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "q_3_4_data_loader = dill.load( open( \"q3_4_loader.p\", \"rb\" ) )\n",
        "train_loader, test_loader = q_3_4_data_loader()\n",
        "q_3_4_net_loader = dill.load( open( \"q3_4_net.p\", \"rb\" ) )\n",
        "net, optimizer = q_3_4_net_loader()\n",
        "new_train_set = torchvision.datasets.MNIST(root='./data_mnist', train=True, \n",
        "                           transform=transforms.ToTensor(), download=True)\n",
        "new_test_set = torchvision.datasets.MNIST(root='./data_mnist', train=False, \n",
        "                           transform=transforms.ToTensor(), download=True)\n",
        "new_train_loader = torch.utils.data.DataLoader(batch_size=100, shuffle=True, dataset=new_train_set)\n",
        "model = train_model(net, optimizer, train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-2e774236b990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_3_4_net_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m new_train_set = torchvision.datasets.MNIST(root='./data_mnist', train=True, \n\u001b[0;32m----> 6\u001b[0;31m                            transform=transforms.ToTensor(), download=True)\n\u001b[0m\u001b[1;32m      7\u001b[0m new_test_set = torchvision.datasets.MNIST(root='./data_mnist', train=False, \n\u001b[1;32m      8\u001b[0m                            transform=transforms.ToTensor(), download=True)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Y2FqPQMhYw6l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3.5 CIFAR diagnosis 2 (10 pts)"
      ]
    },
    {
      "metadata": {
        "id": "wPm-3cGFhGMZ",
        "colab_type": "code",
        "outputId": "18b3e4fa-efe8-46c4-837a-91d569ca5493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "q_3_5_data_loader = dill.load( open( \"q3_5_loader.p\", \"rb\" ) )\n",
        "train_loader, test_loader = q_3_5_data_loader()\n",
        "q_3_5_net_loader = dill.load( open( \"q3_5_net.p\", \"rb\" ) )\n",
        "net,optimizer = q_3_5_net_loader()\n",
        "for g in optimizer.param_groups:\n",
        "    g['lr'] = 0.003\n",
        "model = train_model(net, optimizer, train_loader)\n",
        "# print(optimizer)\n",
        "# print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EpFqb3TT0L37",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3.6 CIFAR diagnosis 3 (10 pts)"
      ]
    },
    {
      "metadata": {
        "id": "r5a5m0wy0v5b",
        "colab_type": "code",
        "outputId": "28bee24c-2bc4-4caf-e157-b46034f889e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "q_3_6_data_loader = dill.load( open( \"q3_6_loader.p\", \"rb\" ) )\n",
        "train_loader, test_loader = q_3_6_data_loader()\n",
        "q_3_6_net_loader = dill.load( open( \"q3_6_net.p\", \"rb\" ) )\n",
        "net, optimizer = q_3_6_net_loader()\n",
        "def init_weights(m):\n",
        "    if type(m) == torch.nn.Linear:\n",
        "        torch.nn.init.kaiming_normal_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "net.apply(init_weights)\n",
        "\n",
        "model = train_model(net, optimizer, train_loader)\n",
        "# print(optimizer)\n",
        "# print(net)\n",
        "# print(net.forward)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7dse7oiK7wPN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q 3.7 MNIST diagnosis 3 (10 pts)"
      ]
    },
    {
      "metadata": {
        "id": "7vA-AFXY0v_B",
        "colab_type": "code",
        "outputId": "11fe3b6d-2ea6-4da3-f080-1d5f1ce56b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "q_3_7_data_loader = dill.load( open( \"q3_7_loader.p\", \"rb\" ) )\n",
        "# q_3_2_data_loader = dill.load( open( \"q3_2_loader.p\", \"rb\" ) )\n",
        "\n",
        "train_loader, test_loader = q_3_7_data_loader()\n",
        "# train_loader, test_loader = q_3_2_data_loader()\n",
        "\n",
        "q_3_7_net_loader = dill.load( open( \"q3_7_net.p\", \"rb\" ) )\n",
        "net, optimizer = q_3_7_net_loader()\n",
        "# model = train_model(net, optimizer, train_loader)\n",
        "print(len(train_loader.dataset))\n",
        "# new_t_loader = torch.utils.data.DataLoader(batch_size=100, shuffle=True, dataset=train_loader.dataset)\n",
        "countarr2 = [0,0,0,0,0,0,0,0,0,0]\n",
        "for i in range (len(train_loader.dataset)):\n",
        "  countarr2[train_loader.dataset[i][1]] = countarr2[train_loader.dataset[i][1]] + 1\n",
        "print(countarr2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "[6005, 6423, 6053, 6085, 5811, 5650, 5959, 6099, 5929, 5986]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9FCcOfA8Tqd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}