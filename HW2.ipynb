{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymann/CIS700-Deep-Learning/blob/master/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GvfC1HN0QDQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deadline\n",
        "\n",
        "This assignment will be due on **April 10, 2019**. Note that this is a group assignment and as such only **one** member per team should submit on canvas."
      ]
    },
    {
      "metadata": {
        "id": "TcAvpMNhZ6c6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone Git\n",
        "\n",
        "Run the following to get the required files needed for this assignment. "
      ]
    },
    {
      "metadata": {
        "id": "L-rDRgbYlvfI",
        "colab_type": "code",
        "outputId": "813ea959-82bd-49bd-ba22-a20c6b7eaa27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/cis700/hw2-release.git\n",
        "!mv hw2-release/* .\n",
        "!rm -rf hw2-release/\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw2-release'...\n",
            "remote: Enumerating objects: 41332, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/41332)   \u001b[K\rremote: Counting objects:   1% (414/41332)   \u001b[K\rremote: Counting objects:   2% (827/41332)   \u001b[K\rremote: Counting objects:   3% (1240/41332)   \u001b[K\rremote: Counting objects:   4% (1654/41332)   \u001b[K\rremote: Counting objects:   5% (2067/41332)   \u001b[K\rremote: Counting objects:   6% (2480/41332)   \u001b[K\rremote: Counting objects:   7% (2894/41332)   \u001b[K\rremote: Counting objects:   8% (3307/41332)   \u001b[K\rremote: Counting objects:   9% (3720/41332)   \u001b[K\rremote: Counting objects:  10% (4134/41332)   \u001b[K\rremote: Counting objects:  11% (4547/41332)   \u001b[K\rremote: Counting objects:  12% (4960/41332)   \u001b[K\rremote: Counting objects:  13% (5374/41332)   \u001b[K\rremote: Counting objects:  14% (5787/41332)   \u001b[K\rremote: Counting objects:  15% (6200/41332)   \u001b[K\rremote: Counting objects:  16% (6614/41332)   \u001b[K\rremote: Counting objects:  17% (7027/41332)   \u001b[K\rremote: Counting objects:  18% (7440/41332)   \u001b[K\rremote: Counting objects:  19% (7854/41332)   \u001b[K\rremote: Counting objects:  20% (8267/41332)   \u001b[K\rremote: Counting objects:  21% (8680/41332)   \u001b[K\rremote: Counting objects:  22% (9094/41332)   \u001b[K\rremote: Counting objects:  23% (9507/41332)   \u001b[K\rremote: Counting objects:  24% (9920/41332)   \u001b[K\rremote: Counting objects:  25% (10333/41332)   \u001b[K\rremote: Counting objects:  26% (10747/41332)   \u001b[K\rremote: Counting objects:  27% (11160/41332)   \u001b[K\rremote: Counting objects:  28% (11573/41332)   \u001b[K\rremote: Counting objects:  29% (11987/41332)   \u001b[K\rremote: Counting objects:  30% (12400/41332)   \u001b[K\rremote: Counting objects:  31% (12813/41332)   \u001b[K\rremote: Counting objects:  32% (13227/41332)   \u001b[K\rremote: Counting objects:  33% (13640/41332)   \u001b[K\rremote: Counting objects:  34% (14053/41332)   \u001b[K\rremote: Counting objects:  35% (14467/41332)   \u001b[K\rremote: Counting objects:  36% (14880/41332)   \u001b[K\rremote: Counting objects:  37% (15293/41332)   \u001b[K\rremote: Counting objects:  38% (15707/41332)   \u001b[K\rremote: Counting objects:  39% (16120/41332)   \u001b[K\rremote: Counting objects:  40% (16533/41332)   \u001b[K\rremote: Counting objects:  41% (16947/41332)   \u001b[K\rremote: Counting objects:  42% (17360/41332)   \u001b[K\rremote: Counting objects:  43% (17773/41332)   \u001b[K\rremote: Counting objects:  44% (18187/41332)   \u001b[K\rremote: Counting objects:  45% (18600/41332)   \u001b[K\rremote: Counting objects:  46% (19013/41332)   \u001b[K\rremote: Counting objects:  47% (19427/41332)   \u001b[K\rremote: Counting objects:  48% (19840/41332)   \u001b[K\rremote: Counting objects:  49% (20253/41332)   \u001b[K\rremote: Counting objects:  50% (20666/41332)   \u001b[K\rremote: Counting objects:  51% (21080/41332)   \u001b[K\rremote: Counting objects:  52% (21493/41332)   \u001b[K\rremote: Counting objects:  53% (21906/41332)   \u001b[K\rremote: Counting objects:  54% (22320/41332)   \u001b[K\rremote: Counting objects:  55% (22733/41332)   \u001b[K\rremote: Counting objects:  56% (23146/41332)   \u001b[K\rremote: Counting objects:  57% (23560/41332)   \u001b[K\rremote: Counting objects:  58% (23973/41332)   \u001b[K\rremote: Counting objects:  59% (24386/41332)   \u001b[K\rremote: Counting objects:  60% (24800/41332)   \u001b[K\rremote: Counting objects:  61% (25213/41332)   \u001b[K\rremote: Counting objects:  62% (25626/41332)   \u001b[K\rremote: Counting objects:  63% (26040/41332)   \u001b[K\rremote: Counting objects:  64% (26453/41332)   \u001b[K\rremote: Counting objects:  65% (26866/41332)   \u001b[K\rremote: Counting objects:  66% (27280/41332)   \u001b[K\rremote: Counting objects:  67% (27693/41332)   \u001b[K\rremote: Counting objects:  68% (28106/41332)   \u001b[K\rremote: Counting objects:  69% (28520/41332)   \u001b[K\rremote: Counting objects:  70% (28933/41332)   \u001b[K\rremote: Counting objects:  71% (29346/41332)   \u001b[K\rremote: Counting objects:  72% (29760/41332)   \u001b[K\rremote: Counting objects:  73% (30173/41332)   \u001b[K\rremote: Counting objects:  74% (30586/41332)   \u001b[K\rremote: Counting objects:  75% (30999/41332)   \u001b[K\rremote: Counting objects:  76% (31413/41332)   \u001b[K\rremote: Counting objects:  77% (31826/41332)   \u001b[K\rremote: Counting objects:  78% (32239/41332)   \u001b[K\rremote: Counting objects:  79% (32653/41332)   \u001b[K\rremote: Counting objects:  80% (33066/41332)   \u001b[K\rremote: Counting objects:  81% (33479/41332)   \u001b[K\rremote: Counting objects:  82% (33893/41332)   \u001b[K\rremote: Counting objects:  83% (34306/41332)   \u001b[K\rremote: Counting objects:  84% (34719/41332)   \u001b[K\rremote: Counting objects:  85% (35133/41332)   \u001b[K\rremote: Counting objects:  86% (35546/41332)   \u001b[K\rremote: Counting objects:  87% (35959/41332)   \u001b[K\rremote: Counting objects:  88% (36373/41332)   \u001b[K\rremote: Counting objects:  89% (36786/41332)   \u001b[K\rremote: Counting objects:  90% (37199/41332)   \u001b[K\rremote: Counting objects:  91% (37613/41332)   \u001b[K\rremote: Counting objects:  92% (38026/41332)   \u001b[K\rremote: Counting objects:  93% (38439/41332)   \u001b[K\rremote: Counting objects:  94% (38853/41332)   \u001b[K\rremote: Counting objects:  95% (39266/41332)   \u001b[K\rremote: Counting objects:  96% (39679/41332)   \u001b[K\rremote: Counting objects:  97% (40093/41332)   \u001b[K\rremote: Counting objects:  98% (40506/41332)   \u001b[K\rremote: Counting objects:  99% (40919/41332)   \u001b[K\rremote: Counting objects: 100% (41332/41332)   \u001b[K\rremote: Counting objects: 100% (41332/41332), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21056/21056), done.\u001b[K\n",
            "remote: Total 41332 (delta 20276), reused 41331 (delta 20275), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (41332/41332), 737.93 MiB | 36.25 MiB/s, done.\n",
            "Resolving deltas: 100% (20276/20276), done.\n",
            "Checking out files: 100% (41168/41168), done.\n",
            "mv: cannot move 'hw2-release/DATASETS' to './DATASETS': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nmHRL-5wWhIQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc \n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.FileWriter(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "        \n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dykqqCddioq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time, datetime\n",
        "now = time.mktime(datetime.datetime.now().timetuple())\n",
        "logger = Logger(f'./logs/2cccc_{now}/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDgmNlaYjl9p",
        "colab_type": "code",
        "outputId": "29005820-e1e0-4b1a-82c3-ad7b93fcf85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ngrok already installed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SUHF57F9jnvv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-cU2hqtMjoFM",
        "colab_type": "code",
        "outputId": "99c5992c-7398-4a55-c30f-3f9475b26528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorboard Link: https://0eb66b08.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dx-Qikt7jyFu",
        "colab_type": "code",
        "outputId": "dfff5500-81b5-4d75-b5c1-69c3547f7076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "  \n",
        "import torch\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.0.1 from https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nx-kHhvzK7Yo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Collaboration Policy**\n",
        "\n",
        "This homework assignment is meant to be done in **groups of 2**. You may work on this individually, but be warned that this homework assignment is extremely long and will be very difficult to do alone. We highly recommend you form groups.\n",
        "\n",
        "You may collaborate with others on a high level, however, all LaTeX and code must be done independently of other groups. Groups who have been shown to be violating this policy will automatically receive a 0 for the assignment and be referred to the Office of Student Conduct. By submitting an assignment you agree that the work produced is your work and your work **only**.\n",
        "\n",
        "**Late Policy**\n",
        "\n",
        "With the exception of emergencies, any homework assignment submitted past the deadline will receive a 20% penalty for each day submitted late.\n",
        "\n",
        "**Online Policy**\n",
        "\n",
        "You may look up guides online that give you general advice / explanations on VAE's / GAN's and may look for instance at the PyTorch documentation but **may not** copy code from anywhere online. We will be transparent that there exists GAN and VAE solutions online (with their respective hyperparameters), however, copying any such code is **strictly prohibited**. We have spent many hours constructing this homework so that you do not have to utilize such resources and as such will be strict in enforcing this policy. Any violations will be directly reported to the OSC. \n"
      ]
    },
    {
      "metadata": {
        "id": "FCEowgppabSx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Classification"
      ]
    },
    {
      "metadata": {
        "id": "AKY6gdpKbT_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 1. Build a CNN Dog Classifier\n",
        "\n",
        "**Understanding the Dataset**\n",
        "\n",
        "For this assignment, we are going to use the Stanford Dogs dataset [link](http://vision.stanford.edu/aditya86/ImageNetDogs/). We are providing a dataloader for you, which can be imported using the following code. Get familiar with the data loader, and visualize some dog pictures. \n",
        "\n",
        "**Q1a (2 pts):** Set the subset parameter for the data loader to 3 and visualize 5 pictures from the training set. Use subset = 3 for the rest of this question unless otherwise stated."
      ]
    },
    {
      "metadata": {
        "id": "xnBvZjeOvcUG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from dogloader import dogs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import time, datetime\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_1HNNeZaiIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = dogs('DATASETS')\n",
        "input_size=224\n",
        "d.transform= transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "d.subset = 3\n",
        "train_images = d.load_split()\n",
        "# image,index = train_images[0]\n",
        "# plt.imshow(mpimg.imread('DATASETS/StanfordDogs/Images/'+image+'.jpg'))\n",
        "# image,index = train_images[1]\n",
        "# plt.imshow(mpimg.imread('DATASETS/StanfordDogs/Images/'+image+'.jpg'))\n",
        "# image,index = train_images[2]\n",
        "# plt.imshow(mpimg.imread('DATASETS/StanfordDogs/Images/'+image+'.jpg'))\n",
        "# image,index = train_images[3]\n",
        "# plt.imshow(mpimg.imread('DATASETS/StanfordDogs/Images/'+image+'.jpg'))\n",
        "# image,index = train_images[4]\n",
        "# plt.imshow(mpimg.imread('DATASETS/StanfordDogs/Images/'+image+'.jpg'))\n",
        "# print(mpimg.imread('DATASETS/StanfordDogs/Images/'+image+'.jpg').view(images.size(0), -1))\n",
        "# # images.view(images.size(0), -1).to(device), labels.to(device)\n",
        "d.train = False\n",
        "test_images = d.load_split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xIT-z0iQdlX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Classification**\n",
        "\n",
        "**Q1b (2 pts):** Use PyTorch to create a Logistic Regression model to classify the Dog Datset. Plot the training curve and report the test accuracy."
      ]
    },
    {
      "metadata": {
        "id": "soQtNqe-q_0t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fix train/ test data\n",
        "for i in range(len(train_images)):\n",
        "  label = train_images[i][1]\n",
        "  img_orig = d.__getitem__(i)[0]\n",
        "#   img_orig = img_orig.resize((100,100))\n",
        "  img = np.array(img_orig).astype(np.float32)\n",
        "  train_images[i] = (img, label)\n",
        "  \n",
        "for i in range(len(test_images)):\n",
        "  label = test_images[i][1]\n",
        "  img_orig = d.__getitem__(i)[0]\n",
        "#   img_orig = img_orig.resize((100,100))\n",
        "  img = np.array(img_orig).astype(np.float32)\n",
        "  test_images[i] = (img, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnFtmcWHP6Xf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class LogRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogRegression, self).__init__()\n",
        "        self.linear = nn.Linear(30000, 120)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ClB-GF2VZxVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "input_size = 30000\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_images, \n",
        "                                          batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_images, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "iter_per_epoch = len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SmynkCbdQ1JR",
        "colab_type": "code",
        "outputId": "91d1c82f-040f-4f8e-e93c-c0a7d007e296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[[-1.6041614 , -1.6041614 , -1.6041614 , ..., -1.004795  ,\n",
            "         -1.004795  , -1.004795  ],\n",
            "        [-1.6041614 , -1.6041614 , -1.6041614 , ..., -1.004795  ,\n",
            "         -1.004795  , -1.004795  ],\n",
            "        [-1.6041614 , -1.6041614 , -1.6041614 , ..., -1.004795  ,\n",
            "         -1.004795  , -1.004795  ],\n",
            "        ...,\n",
            "        [-0.26843056, -0.16568205, -0.06293353, ..., -0.43967807,\n",
            "         -0.33692956, -0.49105233],\n",
            "        [-0.18280679, -0.13143253, -0.09718303, ..., -0.5253019 ,\n",
            "         -0.6109256 , -0.69654936],\n",
            "        [-0.13143253, -0.09718303, -0.11430778, ..., -0.6109256 ,\n",
            "         -0.6622999 , -0.55955136]],\n",
            "\n",
            "       [[-1.6330532 , -1.6330532 , -1.6330532 , ..., -1.177871  ,\n",
            "         -1.177871  , -1.177871  ],\n",
            "        [-1.6330532 , -1.6330532 , -1.6330532 , ..., -1.177871  ,\n",
            "         -1.177871  , -1.177871  ],\n",
            "        [-1.6330532 , -1.6330532 , -1.6330532 , ..., -1.177871  ,\n",
            "         -1.177871  , -1.177871  ],\n",
            "        ...,\n",
            "        [ 0.06512605,  0.17016806,  0.29271722, ...,  0.08263306,\n",
            "          0.18767506,  0.03011205],\n",
            "        [ 0.13515405,  0.2051822 ,  0.2752102 , ..., -0.02240895,\n",
            "         -0.10994396, -0.19747896],\n",
            "        [ 0.2401962 ,  0.2752102 ,  0.2752102 , ..., -0.12745096,\n",
            "         -0.17997196, -0.07492995]],\n",
            "\n",
            "       [[-1.7172985 , -1.7172985 , -1.7172985 , ..., -1.5778649 ,\n",
            "         -1.5778649 , -1.5778649 ],\n",
            "        [-1.7172985 , -1.7172985 , -1.7172985 , ..., -1.5778649 ,\n",
            "         -1.5778649 , -1.5778649 ],\n",
            "        [-1.7172985 , -1.7172985 , -1.7172985 , ..., -1.5778649 ,\n",
            "         -1.5778649 , -1.5778649 ],\n",
            "        ...,\n",
            "        [ 0.06047938,  0.13019615,  0.23477131, ...,  0.04305018,\n",
            "          0.14762534, -0.0092374 ],\n",
            "        [ 0.2696297 ,  0.32191727,  0.37420484, ...,  0.0081918 ,\n",
            "         -0.07895417, -0.16610013],\n",
            "        [ 0.42649257,  0.44392177,  0.42649257, ..., -0.06152498,\n",
            "         -0.11381256, -0.0092374 ]]], dtype=float32), 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "agJXw66sSCNB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, input_size):\n",
        "  now = time.mktime(datetime.datetime.now().timetuple())\n",
        "  logger = Logger(f'./logs/2cccc_{now}/')\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "  # img = np.array(train_images[0][0])\n",
        "  # print(img.view(img.size, -1))\n",
        "  # print(train_images[0][0])\n",
        "  # print(train_images[1][0].size)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "  # #         print(images)\n",
        "  #         print(images.view(images.size(0), -1))\n",
        "  #     print(images[0].type())\n",
        "      images, labels = images.view(-1,input_size).to(device), labels.to(device)\n",
        "  #     print(\"b: \", images.size())\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "       # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "\n",
        "      # ================================================================== #\n",
        "      #                        Tensorboard Logging                         #\n",
        "      # ================================================================== #\n",
        "\n",
        "      # 1. Log scalar values (scalar summary)\n",
        "      info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
        "      for tag, value in info.items():\n",
        "        logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N717Q7aTdAQm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(model, input_size):\n",
        "  criterion = torch.nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "\n",
        "      images, labels = images.view(-1,input_size).to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "       # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "\n",
        "#       # ================================================================== #\n",
        "#       #                        Tensorboard Logging                         #\n",
        "#       # ================================================================== #\n",
        "\n",
        "#       # 1. Log scalar values (scalar summary)\n",
        "#       info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
        "#       for tag, value in info.items():\n",
        "#         logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oHXMnJjbfKEL",
        "colab_type": "code",
        "outputId": "8ec1e3f1-f5d5-47ab-be94-9b2b07ba342c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "cell_type": "code",
      "source": [
        "model = LogRegression().to(device)\n",
        "train_model(model, 30000)\n",
        "test_model(model,30000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-671142adeee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-1e0d3f4c0d8d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, input_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m#         print(images.view(images.size(0), -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#     print(images[0].type())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m#     print(\"b: \", images.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 30000]' is invalid for input of size 15052800"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wcmwXGEfPQNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Feed Forward Neural Network Classification**\n",
        "\n",
        "**Q1c (3 pts):** Create a Feed Forward neural network to classify the Dog Dataset. Plot the training curve and report the test accuracy."
      ]
    },
    {
      "metadata": {
        "id": "ztwdjtouReTE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(30000, 10000)\n",
        "        self.fc2 = nn.Linear(10000, 150)\n",
        "        self.fc3 = nn.Linear(150, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LUTJvSt5hlp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = NN().to(device)\n",
        "train_model(model, 30000)\n",
        "test_model(model,30000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3qGoYKGRe4l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CNN Classification**\n",
        "\n",
        "**Q1c (3 pts):** Create a CNN to classify the Dog Dataset. Plot the training curve and report the test accuracy."
      ]
    },
    {
      "metadata": {
        "id": "tBa6yH3z3_ho",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model_cnn(model):\n",
        "  now = time.mktime(datetime.datetime.now().timetuple())\n",
        "  logger = Logger(f'./logs/2cccc_{now}/')\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      images, labels = images.view(-1,3,100,100).to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "       # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "\n",
        "      # ================================================================== #\n",
        "      #                        Tensorboard Logging                         #\n",
        "      # ================================================================== #\n",
        "\n",
        "      # 1. Log scalar values (scalar summary)\n",
        "      info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
        "      for tag, value in info.items():\n",
        "        logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O8V2VULJ3_Q7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model_cnn(model):\n",
        "  criterion = torch.nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "\n",
        "      images, labels = images.view(-1,3,100,100).to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "       # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "\n",
        "#       # ================================================================== #\n",
        "#       #                        Tensorboard Logging                         #\n",
        "#       # ================================================================== #\n",
        "\n",
        "#       # 1. Log scalar values (scalar summary)\n",
        "#       info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
        "#       for tag, value in info.items():\n",
        "#         logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjQNQDAUR2W5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN(torch.nn.Module):\n",
        "        \n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3,32,kernel_size=3,stride=1,padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32,64,kernel_size=3,stride=1,padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256*7*7, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fv61ZmPeWJB5",
        "outputId": "0391022f-ab5a-4504-e4a2-3418b007b8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "train_model_cnn(model)\n",
        "test_model_cnn(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0200, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "metadata": {
        "id": "aOSlq0NjPYLW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q1d (5 pts):** Use the same architecture from the previous question (except for the last linear layer--change the output number of classes), but set the subset = 120 this time. Train a new model, plot your training accuracy and report your final test accuracy. What do notice about the training and testing accuracy? Keep a note of your final testing accuracy. "
      ]
    },
    {
      "metadata": {
        "id": "7yOZZ1IoPbmV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "input_transforms = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RM98AVP_6P1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res_num_epochs = 5\n",
        "res_batch_size = 50\n",
        "learning_rate = 0.0001\n",
        "\n",
        "dog_train_dataset = dogs('DATASETS', transform=input_transforms, train=True)\n",
        "dog_test_dataset = dogs('DATASETS', transform=input_transforms, train=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=dog_train_dataset, \n",
        "                                               batch_size=res_batch_size, \n",
        "                                               shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=dog_test_dataset, \n",
        "                                               batch_size=res_batch_size, \n",
        "                                               shuffle=True)\n",
        "\n",
        "iter_per_epoch = len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_d9JCzDJPdhF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "train_model_cnn(model)\n",
        "test_model_cnn(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PizY5LtOhLz_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 2. Transfer Learning on ResNets for Image Classification \n",
        "\n",
        "In this question we will explore different ways to bootstrap your image classifier when training examples are scarce. For the dog dataset, we have around 100 training images for each class, and variation between each class is limited (all dogs). Due to the limited number of training examples, building a high-performing image classifier is challenging. \n",
        "\n",
        "ImageNet is one of the most popular computer vision data sets. It  contains 10 million images in total from multiple different categories. The Stanford Dogs dataset we are using is a subset from ImageNet, orginally intended for fine-grained object classification tasks. PyTorch provides ResNets that are pre-trained on ImageNet, and using this pretrained model, we can build a dog classifier much more easily. \n",
        "\n",
        "**Note:** In Question 2 set subset = 120 (the default).\n",
        "\n",
        "**Q2 (15 pts):** In the code snippet below, load a pre-trained ResNet model and swap out the last fully connected layer from the network with your own classification layer. Train the network, plot your training accuracy, and report your final test accuracy. Your test accuracy should be at least 70%. A few tips:\n",
        "\n",
        "* After the last fully connected layer of the network, remember to freeze the network except for the last layer to speed up the training.\n",
        "\n",
        "* Pretrained ResNet models assumes that input is normalized in the following fashion, and is of the size at least 224."
      ]
    },
    {
      "metadata": {
        "id": "OMcM9CscVA0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "input_transforms = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6N7i_ca3-AIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res_num_epochs = 5\n",
        "res_batch_size = 50\n",
        "learning_rate = 0.0001\n",
        "\n",
        "dog_train_dataset = dogs('DATASETS', transform=input_transforms, train=True)\n",
        "dog_test_dataset = dogs('DATASETS', transform=input_transforms, train=False)\n",
        "\n",
        "res_train_loader = torch.utils.data.DataLoader(dataset=dog_train_dataset, \n",
        "                                               batch_size=res_batch_size, \n",
        "                                               shuffle=True)\n",
        "res_test_loader = torch.utils.data.DataLoader(dataset=dog_test_dataset, \n",
        "                                               batch_size=res_batch_size, \n",
        "                                               shuffle=True)\n",
        "\n",
        "iter_per_epoch = len(res_train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kqmO3sML-GhF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model_res(model):\n",
        "  now = time.mktime(datetime.datetime.now().timetuple())\n",
        "  logger = Logger(f'./logs/2cccc_{now}/')\n",
        "  criterion = torch.nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "  for epoch in range(res_num_epochs):\n",
        "    for i, (images, labels) in enumerate(res_test_loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "       # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "      \n",
        "      #       # ================================================================== #\n",
        "#       #                        Tensorboard Logging                         #\n",
        "#       # ================================================================== #\n",
        "\n",
        "#        #1. Log scalar values (scalar summary)\n",
        "      \n",
        "#       info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
        "#       for tag, value in info.items():\n",
        "#         logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H_tNDzZOg5qy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def train_model_res(model):\n",
        "  now = time.mktime(datetime.datetime.now().timetuple())\n",
        "  logger = Logger(f'./logs/2cccc_{now}/')\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)  \n",
        "\n",
        "  for epoch in range(res_num_epochs):\n",
        "    for i, (images, labels) in enumerate(res_train_loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "    \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "       # Compute accuracy\n",
        "      _, argmax = torch.max(outputs, 1)\n",
        "      accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "      print(loss)\n",
        "\n",
        "      # ================================================================== #\n",
        "      #                        Tensorboard Logging                         #\n",
        "      # ================================================================== #\n",
        "\n",
        "      # 1. Log scalar values (scalar summary)\n",
        "      info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
        "      for tag, value in info.items():\n",
        "        logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZQ40_8f-Lub",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# print(resnet18.__dict__) # ('fc', Linear(in_features=512, out_features=1000, bias=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for params in resnet18.parameters():\n",
        "   params.requires_grad = False\n",
        "    \n",
        "# Substitute last layer with unfrozen linear unit\n",
        "resnet18.fc = nn.Linear(in_features=resnet18.fc.in_features, out_features=120)\n",
        "resnet18.train()\n",
        "train_model_res(resnet18.to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GaVJv_im-TWW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resnet18.eval()\n",
        "test_model_res(resnet18.to(device)) # gave test accuracy of exactly 70 lol"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Exrju8Q2VBn1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 3. Generative Models: Convolutional Autoencoders and Variational Autoencoders\n",
        "\n",
        "In this question you will create a Convolutional AutoEncoder (CAE) and a Variational AutoEncoder (VAE) for the Stanford Dogs Dataset. Before you em**bark** on this part, please read the [slides](https://www.seas.upenn.edu/~cis700dl/slides/7M.pptx) thoroughly to ensure that you understand how an autoencoder works (i.e. the loss function and general structure). The slides provide some sample code you follow, but note that you may need to create a bigger network. Also, the sample code on the slides only output a black and white image, but you should create a network that output color images. \n",
        "\n",
        "To get good reconstruction results, the AE might take a long time to train (for instance > 2 hours); however, you should be able to tell whether or not your network is learning much earlier than that. Be sure to not just look at your loss, but also periodically examine your image outputs to ensure that they are becoming sharper and sharper representations of your input image. The choice of when to stop your AE's training is up to your discretion (however the output should be reasonably close). \n",
        "\n",
        "**Note:** In this section set subset = 120 (the default)\n",
        "\n",
        "**Q3a (10 pts):** After creating your AE, plot the resulting training curve and in your report give 5 examples of the input and corresponding reconstruction.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Mbs6orwQVAxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class cae(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(cae, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3,192,kernel_size=2,stride=1,padding=1), #(224,224)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(112,112)\n",
        "            nn.Conv2d(192,96,kernel_size=3,stride=3,padding=1), #(38,38)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(19,19)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(96, 192, kernel_size=2, stride=3, padding=1), #(54,54)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2), #(111,111)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(96, 3, kernel_size=4, stride=2), #(224,224)\n",
        "            nn.Tanh()\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "#         print(x.size())\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 3, 224, 224)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-U5I-3eCuXgE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf dc_img\n",
        "!mkdir dc_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "waOqj_noxYZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "now = time.mktime(datetime.datetime.now().timetuple())\n",
        "logger = Logger(f'./logs/2cccc_{now}/')\n",
        "num_epochs=30\n",
        "model = cae().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                             weight_decay=1e-5)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        img, _ = data\n",
        "        img = Variable(img).to(device)\n",
        "        # ===================forward=====================\n",
        "        output = model(img)\n",
        "        loss = criterion(output, img)\n",
        "        # ===================backward====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # ===================log========================\n",
        "    print('epoch [{}/{}], loss:{:.4f}'\n",
        "          .format(epoch+1, num_epochs, loss.data))\n",
        "    pic = to_img(output.cpu().data)\n",
        "    save_image(pic, './dc_img/image_{}.png'.format(epoch))\n",
        "        \n",
        "    \n",
        "#      # Compute accuracy\n",
        "#     _, argmax = torch.max(outputs, 1)\n",
        "#     accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "\n",
        "    # ================================================================== #\n",
        "    #                        Tensorboard Logging                         #\n",
        "    # ================================================================== #\n",
        "\n",
        "    # 1. Log scalar values (scalar summary)\n",
        "    info = { 'loss': loss.item() }\n",
        "    for tag, value in info.items():\n",
        "      logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)\n",
        "\n",
        "torch.save(model.state_dict(), './conv_autoencoder.pth')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xnoWdA3TFiB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuiQ5UIMqQxL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q3b (5 pts):** An interesting property of AE's is that the \"code\" vector has additive and subtractive properties. Perform addition and subtraction on different code vectors and pass it through your decoder to create a new image. Explain why the image looks as it does. "
      ]
    },
    {
      "metadata": {
        "id": "N2VaAkbuMZo5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class cae(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(cae, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3,192,kernel_size=2,stride=1,padding=1), #(224,224)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(112,112)\n",
        "            nn.Conv2d(192,96,kernel_size=3,stride=3,padding=1), #(38,38)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(19,19)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(96, 192, kernel_size=2, stride=3, padding=1), #(54,54)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2), #(111,111)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(96, 3, kernel_size=4, stride=2), #(224,224)\n",
        "            nn.Tanh()\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.encoder(x)\n",
        "        y = self.encoder(y)\n",
        "#         print(x.size())\n",
        "        x = self.decoder(x+y)\n",
        "        return x\n",
        "\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 3, 224, 224)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fiqq_mi2mKbk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q3c (10 pts optional extra credit)**: Using an autoencoder can be a great way to get increased classification accuracy without actually needing more data. For extra credit, use the AE that you just constructed to seed the network and then train on all 120 classes of the Stanford Dog's Dataset. Plot the training curve and test accuracy \n",
        "\n",
        "*(Hint: Employ a similar strategy to ResNet to remove the last layer of your AE and instead replace it with a fully connected layer of size 120)*"
      ]
    },
    {
      "metadata": {
        "id": "zP8asaIUq8my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class cae(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(cae, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3,192,kernel_size=2,stride=1,padding=1), #(224,224)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(112,112)\n",
        "            nn.Conv2d(192,96,kernel_size=3,stride=3,padding=1), #(38,38)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(19,19)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(96, 192, kernel_size=2, stride=3, padding=1), #(54,54)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2), #(111,111)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(96, 3, kernel_size=4, stride=2), #(224,224)\n",
        "            nn.Tanh()\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "#         print(x.size())\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 3, 224, 224)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2u4Zo0dwssMW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q3d (20 pts):** Variational AutoEncoders, as covered in class, learn the probability distribution of the inputs and can be used to generate novel images.  We can further pass convolutional features into a VAE to create a convolutional VAE.  Convert your CAE from the previous question to a convolutional VAE by executing the following steps:\n",
        "\n",
        "* In the encoder, generate two tensors of the same length, representing mean and standard deviation of the latent probability distribution. \n",
        "\n",
        "* Sample from a normal distribution from the learned mean and std to generate the final encoding tensor -- this is the reparametrization trick discussed in lecture.\n",
        "\n",
        "* Decode from the encoding tensor just as in the convolutional autoencoder. \n",
        "\n",
        "* Change your loss function from MSE reconstruction loss to the objective function for VAE:\n",
        " * Reconstrucation Loss: binary_cross_entropy loss between original and reconstructed image. \n",
        " * Regularization on the sampled latent normal distribution \n",
        "\n",
        "$$ L_{reconstruciton} = -\\frac{1}{n} \\sum_{i}^{n}(x_i log(f(z_i)) + (1-x_i) log(1-f(z_i))) $$\n",
        "$$ L_{regularization} = \\frac{1}{2n}\\sum_{i}^{n}(\\mu_{i}^{2} + \\sigma_{i}^2 - log(\\sigma_i^2)- 1)$$\n",
        "$$ L_{loss} =L_{regularization} +  L_{reconstruction}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Zb0rKLR_RddW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "input_transforms = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fiaKqGCmRddY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res_num_epochs = 5\n",
        "res_batch_size = 50\n",
        "learning_rate = 0.0001\n",
        "\n",
        "dog_train_dataset = dogs('DATASETS', transform=input_transforms, train=True)\n",
        "dog_test_dataset = dogs('DATASETS', transform=input_transforms, train=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=dog_train_dataset, \n",
        "                                               batch_size=res_batch_size, \n",
        "                                               shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=dog_test_dataset, \n",
        "                                               batch_size=res_batch_size, \n",
        "                                               shuffle=True)\n",
        "\n",
        "iter_per_epoch = len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-0cdN70gPAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class vae(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(vae, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3,16,kernel_size=4,stride=2,padding=1), #(111,111)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(55,55)\n",
        "            nn.Conv2d(16,32,kernel_size=3,stride=2,padding=1), #(28,28)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(14,14)\n",
        "            nn.Conv2d(32,64,kernel_size=2,stride=1,padding=1), #(15,15)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(7,7)\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1), #(4,4)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), #(2,2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(2*2*128, 128*10*10)\n",
        "        self.fc2 = nn.Linear(2*2*128, 128*10*10)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=3, padding=1), #(29,29)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2, padding=1), #(56,56)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=1), #(113,113)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=2, stride=2, padding=1), #(224,224)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "    def encode(self, x):\n",
        "      out = self.encoder(x)\n",
        "      out = out.view(out.size(0), -1)\n",
        "      return self.fc1(out), self.fc2(out)\n",
        "      \n",
        "    def reparametrize(self, mu, logvar):\n",
        "      std = logvar.mul(0.5).exp_()\n",
        "#       eps = torch.FloatTensor(std.size()).normal_()\n",
        "      eps = torch.FloatTensor(std.size()).normal_().to(device)\n",
        "      to_return = eps.mul(std).add_(mu)\n",
        "      return to_return.view(to_return.size(0), -1, 10, 10)\n",
        "       \n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 3, 224, 224)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_yKKnI4OFCWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def criterion_vae(output, img, mu, logvar):\n",
        "  criterion1 = nn.BCELoss()\n",
        "  l_recon = criterion1(output, img)\n",
        "  l_reg = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "  return l_recon + l_reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1wE2h4ycDcFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf vae\n",
        "!mkdir vae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-PZ7tm1i3hEO",
        "colab_type": "code",
        "outputId": "a032d56d-0c69-430f-c400-3fed1a704c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "now = time.mktime(datetime.datetime.now().timetuple())\n",
        "logger = Logger(f'./logs/2cccc_{now}/')\n",
        "num_epochs=15\n",
        "# model = vae()\n",
        "model = vae().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                             weight_decay=1e-5)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        img, label = data\n",
        "#         img = Variable(img)\n",
        "#         print(img[0])\n",
        "        img = Variable(img).to(device)\n",
        "        # ===================forward=====================\n",
        "        output, mu, logvar = model(img)\n",
        "#         print(img.size(), output.size())\n",
        "#         print(img[0].min())\n",
        "#         print(output[0])\n",
        "        loss = criterion_vae(output, img, mu, logvar)\n",
        "        # ===================backward====================\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # ===================log========================\n",
        "    print('epoch [{}/{}], loss:{:.4f}'\n",
        "          .format(epoch+1, num_epochs, loss.data))\n",
        "    pic = to_img(output.cpu().data)\n",
        "    save_image(pic, './vae/image_{}.png'.format(epoch))\n",
        "        \n",
        "    \n",
        "#      # Compute accuracy\n",
        "#     _, argmax = torch.max(outputs, 1)\n",
        "#     accuracy = (labels == argmax.squeeze()).float().mean()\n",
        "\n",
        "    # ================================================================== #\n",
        "    #                        Tensorboard Logging                         #\n",
        "    # ================================================================== #\n",
        "\n",
        "    # 1. Log scalar values (scalar summary)\n",
        "    info = { 'loss': loss.item() }\n",
        "    for tag, value in info.items():\n",
        "      logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)\n",
        "\n",
        "torch.save(model.state_dict(), './conv_autoencoder.pth')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch [1/15], loss:0.6816\n",
            "epoch [2/15], loss:0.6615\n",
            "epoch [3/15], loss:0.6494\n",
            "epoch [4/15], loss:0.6392\n",
            "epoch [5/15], loss:0.6343\n",
            "epoch [6/15], loss:0.6299\n",
            "epoch [7/15], loss:0.6362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YCImqg4OoaYZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6t7LNS5HJmDv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Question 4. Generative Models: GAN's"
      ]
    },
    {
      "metadata": {
        "id": "NsgUGRMLJr11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this question you will create a Generative Adversarial Network (GAN) for the Stanford Dogs Dataset. Before you embark on this part, please read the [slides](https://www.seas.upenn.edu/~cis700dl/slides/7W.pptx) thoroughly to ensure that you understand how the multiple loss functions and general structure of a GAN works. The slides provide some sample code you follow, but note that you may need to create a bigger network and that the slides only output a black and white image, but you should create a network that output color images. \n",
        "\n",
        "GANs are notoriously difficult to train, and as such we do not expect that you get perfect looking dogs even after multiple hours of training. In the dataset provided there's a great deal of variation between dog breeds which can make it difficult to get good results from our simplistic GAN approach. If implemented correctly, you should be getting results which look blurry but vaguely dog-like.\n",
        "\n",
        "![Dog1](https://imgur.com/76dpyLt.png)\n",
        "![Dog2](https://imgur.com/z5Sldte.png)\n",
        "![Dog3](https://i.imgur.com/RferuyY.png)\n",
        "![Dog4](https://i.imgur.com/t9Rrbga.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QkDd4lUnxtdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4a (2 pts):** Create a normally distributed vector $z$ with $\\mu = 0$, $\\sigma = 1$ (i.e the np.random.normal default). The vector should have size [batch size, feature length, 1, 1]. You don't need to submit anything for this part. Here feature length describes the length of your vector $z$.\n"
      ]
    },
    {
      "metadata": {
        "id": "Eqtp2-5Py1hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gan_epochs = 2\n",
        "gan_batch_size = 25\n",
        "gan_feature_length = 100\n",
        "\n",
        "gan_input_size = 64\n",
        "gan_input_transforms = transforms.Compose([\n",
        "            transforms.Resize((gan_input_size, gan_input_size)),\n",
        "            transforms.ToTensor()])\n",
        "\n",
        "# transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "gan_dog_train_dataset = dogs('DATASETS', transform=gan_input_transforms, train=True)\n",
        "gan_dog_test_dataset = dogs('DATASETS', train=False)\n",
        "\n",
        "gan_train_loader = torch.utils.data.DataLoader(dataset=gan_dog_train_dataset, \n",
        "                                               batch_size=gan_batch_size, \n",
        "                                               shuffle=True)\n",
        "gan_test_loader = torch.utils.data.DataLoader(dataset=gan_dog_test_dataset,\n",
        "                                               batch_size=gan_batch_size, \n",
        "                                               shuffle=True)\n",
        "\n",
        "iter_per_epoch = len(gan_train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6IG88nMXAkDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def genNormVector(batch_size, feature_length):\n",
        "  return torch.from_numpy(np.random.normal(size=(batch_size, feature_length, 1, 1)).astype(np.float32)).to(device)\n",
        "\n",
        "z = torch.from_numpy(np.random.normal(size=(gan_batch_size, gan_feature_length, 1, 1)).astype(np.float32)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VhhBlx1xy52L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4b (2 pts):** Create the generator network. Use [ConvTranspose2d](https://pytorch.org/docs/stable/nn.html) to upsample the noise vector $z$ to a size of your choosing (512 is what we used but you can/should tune this). Deconvolve it until the number of channels is 3 (so it's RGB), and the output size is (64,64).  In other words, the output of the generator should be [batch size,3,64,64]. Use ReLU and batch norm after every deconvolution and use a sigmoid layer at the end to create your final output. Don't worry about the loss function for now. Describe what your final network looks like in your writeup and why you made these choices."
      ]
    },
    {
      "metadata": {
        "id": "jqV95mQi7B-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Generator, self).__init__()\n",
        "      \n",
        "      def block(in_feat, out_feat, kernel, mstride=1, padding = 0, opadding=0):\n",
        "        layers = [nn.ConvTranspose2d(in_feat, out_feat, kernel_size=kernel, stride=mstride, output_padding=opadding, padding=padding)]\n",
        "        layers.append(nn.BatchNorm2d(out_feat))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        return layers\n",
        "      \n",
        "      self.model = nn.Sequential(\n",
        "          *block(512, 256, kernel=5, mstride = 3, padding=1),\n",
        "          *block(256, 128, kernel=5, mstride = 2, padding=1),\n",
        "          *block(128, 64, kernel=5 , mstride = 2, padding=1),\n",
        "          *block(64, 3, kernel=4, mstride = 1, padding=1),\n",
        "          \n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "      \n",
        "      self.conv = nn.ConvTranspose2d(gan_feature_length, 512, kernel_size=5, stride=1, output_padding = 0)    \n",
        "       \n",
        "\n",
        "    def forward(self, x):\n",
        "      aconv = self.conv(x)\n",
        "      img = self.model(aconv)\n",
        "      return img\n",
        "\n",
        "generator = Generator().to(device)\n",
        "print(z.shape)\n",
        "print(generator(z).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "erYhxjMlOqag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4c (2 pts):** Create the discriminator network. This should feel very similar to creating a CNN to classify whether an example is in the distribution or not in the distribution. Don't worry about the loss function for this part. Describe what your final network looks like in your writeup and why you made these choices."
      ]
    },
    {
      "metadata": {
        "id": "ONmO06uI7CVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3,128,kernel_size=5),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128,kernel_size=5),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 1, kernel_size=3)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(15000, 1)\n",
        "        self.drop = nn.Dropout2d(0.4)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "#         out = out.reshape(out.size(0), -1)\n",
        "#         out = self.fc1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.sig(out)\n",
        "        return(out)\n",
        "\n",
        "goutput = generator(z)\n",
        "print(goutput.shape)\n",
        "discriminator = Discriminator().to(device)\n",
        "print(discriminator(goutput).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kkplZqRKA1ZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xC5eUaPn2fjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4d (3 pts):** Code the loss function for the generator, it should be as follows:\n",
        "\n",
        "$$ L_G = -\\frac{1}{n}\\sum_{i=1}^{n}\\lg D(G(z))$$\n",
        "\n",
        "An alternate form for convenince is:\n",
        "$$ L_G = \\frac{1}{n}\\sum_{i=1}^{n}L_{CE}(D(G(z)), 1)$$\n",
        "\n",
        "Where $L_{CE}$ is the cross entropy loss function and n is the batch size. Submit a screenshot (or use mcode) to include your implementation in the writeup. Why is the ground truth label for fake data 1 here? Explain your answer in the write-up.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NNs1DqZa7C_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator_loss(batch_size, gen_output, discriminator):\n",
        "  \n",
        "  criterion = nn.BCELoss()\n",
        "\n",
        "  doutput = discriminator(gen_output)\n",
        "  ones = torch.ones(doutput.shape, dtype=torch.float, device=device)\n",
        "  \n",
        "  loss = criterion(doutput, ones)\n",
        "\n",
        "#   loss = torch.sum(torch.log(doutput))   \n",
        "#   loss = (-1*(1/batch_size)*loss)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nHrNy9f54F6d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4e (3 pts):** Code the loss function for the generator, it should be as follows:\n",
        "\n",
        "$$ L_D = \\frac{1}{2n}\\sum_{i=1}^{n}L_{CE}(D(X_i), 1) + L_{CE}(D(G(z)), 0)$$\n",
        "\n",
        "Where $L_{CE}$ is the cross entropy loss function and n is the batch size. Be sure that you're not normalizing by $n$ twice (if you feed in two vectors into BCELoss, they normalize it by the size of the vector for you).\n",
        "\n",
        "**Important:** Remember to **detach** your generator when calculating this loss. Think about why this is the case and the repercussions of not doing so. Detail the answer to this question in your write-up. Submit a screenshot (or use mcode) of your implementation in the writeup."
      ]
    },
    {
      "metadata": {
        "id": "-LwuAUDJ7Dhi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def discriminator_loss(batch_size, batch_images, fake_images, discriminator, iterations):\n",
        "  # add assert here to confirm len(xs) == batch_size\n",
        " \n",
        "  criterion1 = nn.BCELoss()\n",
        "  criterion2 = nn.BCELoss()\n",
        "  \n",
        "  \n",
        "  dx = discriminator(batch_images)\n",
        "  dg = discriminator(fake_images.detach())\n",
        "  \n",
        "  # loss1 = real image loss, loss2 = fake image loss\n",
        "  \n",
        "  cz = random.randint(0, 20)/100\n",
        "  co = random.randint(80, 100)/100\n",
        "  \n",
        "  # Flip labels every 200 iterations\n",
        "  if (iterations % 200 == 0):\n",
        "    real_loss_tensor = torch.full(size=dx.shape, fill_value = cz, dtype=torch.float, device=device)\n",
        "    fake_loss_tensor = torch.full(size=dg.shape, fill_value = co, dtype=torch.float, device=device)\n",
        "  else:\n",
        "    real_loss_tensor = torch.full(size=dx.shape, fill_value = co, dtype=torch.float, device=device)\n",
        "    fake_loss_tensor = torch.full(size=dg.shape, fill_value = cz, dtype=torch.float, device=device)\n",
        "   \n",
        "  \n",
        "  real_loss = criterion1(dx, real_loss_tensor)\n",
        "  fake_loss = criterion2(dg, fake_loss_tensor)\n",
        "\n",
        "#   real_loss = real_loss * (1 / (2*batch_size))\n",
        "#   fake_loss = fake_loss * (1 / (2*batch_size))\n",
        "  \n",
        "  loss = (real_loss + fake_loss)/2\n",
        "   \n",
        "  return loss, real_loss, fake_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mexodpuXEva0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "pcu5cqGt61Wx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4f (3 pts):** Create two optimizers, one for your Discriminator network and one for your generator network. Describe your choices for the learning rate and optimizer in the writeup."
      ]
    },
    {
      "metadata": {
        "id": "xv_XQqqU7EDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator_lr = 0.0005\n",
        "discriminator_lr = 0.0005\n",
        "\n",
        "def optimizers(generator, discriminator):\n",
        "  gen_optimizer = torch.optim.Adam(generator.parameters(), lr=generator_lr)\n",
        "  dis_optimizer = torch.optim.Adam(discriminator.parameters(), lr=discriminator_lr)  \n",
        "  \n",
        "  return gen_optimizer, dis_optimizer\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lgxgndh7Ayo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4g (20 pts):** Put all of these parts together to create a GAN. The training loop should look as follows:\n",
        "\n",
        "\n",
        "\n",
        "1.   Create your vector $z$\n",
        "2.   Zero out the gradient for $G$\n",
        "3.   Generate your fake image, and calculate $L_G$\n",
        "4.   Backpropagate $L_G$ and have your optimizer take a step\n",
        "5.   Zero out the gradient for $D$\n",
        "6.   Calculate $L_D$\n",
        "7.   Backpropagate $L_D$ and step your optimizer.\n",
        "\n",
        "Hints for training:\n",
        "\n",
        "1.   Make sure you're using tensorboard to keep track of the discriminator AND generator losses throughout. Note that if your discriminator loss goes to 0, this represents a failure mode of your training (look at how the generator loss function is calculated and you realize that G's gradients vanish).\n",
        "2.   Keep vigilant for mode collapse. Display your image after every delta number of iterations to see that not only is your image getting better, but you're also not getting the same image each time. If so, then your network is suffering from mode collapse.\n",
        "3.   Analyze your generator loss and make sure that it's roughly oscillating around a certain loss. Unlike most other tasks, your generator and discriminator loss are NOT intended to monotonically decrease. Instead they should bounce around a certain loss (see image below for example). Your graph doesn't have to look like this but this is meant to illustrate how the loss should roughly bounce around a certain loss.\n",
        "![Sample Loss Graph](https://cdn-images-1.medium.com/max/1600/1*4A5bo8gVG9wmg-5wtqavOg.png)\n",
        "\n",
        "What makes GAN's difficult to train is that they are extremely sensitive to hyperparameters. Some hyperparameters to think about when tuning your GAN:\n",
        "\n",
        "\n",
        "1.   The number of times you run your discriminator vs. your generator (generally you run your discriminator more times than your generator because the discriminator is what gives your generator feedback).\n",
        "2.   The learning rate for your discriminator / generator\n",
        "3.   Your image output size\n",
        "4.   Your feature vector size (i.e. the length of z)\n",
        "5.   Your standard neural network hyperparameters (i.e. number of layers, width of layers, batch size, etc.)\n",
        "\n",
        "\n",
        "\n",
        "We're going to overall be pretty generous with how your outputs look for this part (i.e. does it look somewhat dog-like), however, you can look to lots of other sources for how to improve your GAN performance / understand GANs better:\n",
        "\n",
        "\n",
        "\n",
        "1.   [Keep Calm and train a GAN](https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9)\n",
        "2.   [Why is it so hard to train GAN's!](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n",
        "\n",
        "And many more!\n",
        "\n",
        "We will be awarding generous extra credit to those with particularly good GAN results. \n",
        "\n",
        "Be sure to start this part early as you will definitely run into issues with training that you wouldn't originally forsee!\n",
        "\n",
        "After you finish creating your GAN, give us 5 of your best outputs (not all of your results will look reasonable) and plot your training curve for both the generator AND discriminator. Report your final hyper-parameter choices that weren't already reported above (i.e. the batch size, model parameters, etc.)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IAV4dzCgDO0H",
        "colab_type": "code",
        "outputId": "80620eb2-df84-4c61-b873-25fffcd9d5dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir gans"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory gans: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SzuNsytLBCBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "def gan_training_loop(generator, discriminator, epochs):\n",
        "  gen_optimizer, dis_optimizer = optimizers(generator, discriminator)\n",
        "  counter = 0\n",
        "  dcounter = 0\n",
        "  for epoch in range(epochs):\n",
        "    for i, (real_images, labels) in enumerate(gan_train_loader):\n",
        "      real_images, labels = real_images.to(device), labels.to(device)\n",
        "      \n",
        "      ## TRAIN GENERATOr\n",
        "      z = genNormVector(gan_batch_size, gan_feature_length) # 1. Create your vector z\n",
        "      \n",
        "      gen_optimizer.zero_grad() # 2. Zero out gradient for G\n",
        "      \n",
        "      fake_images = generator(z).to(device) # 3 Generate your fake image and get loss\n",
        "      g_loss = generator_loss(gan_batch_size, fake_images, discriminator)\n",
        "            \n",
        "      g_loss.backward()\n",
        "      \n",
        "#       for name, parameter in generator.named_parameters():\n",
        "#         print(torch.mean(parameter.grad))\n",
        "#         break\n",
        "        \n",
        "      gen_optimizer.step()\n",
        "      \n",
        "      \n",
        "      info = { 'gloss': g_loss.item(), }\n",
        "      for tag, value in info.items():\n",
        "        logger.scalar_summary(tag, value, (epoch) * iter_per_epoch + i+1)  \n",
        "  \n",
        "  \n",
        "      ## TRAIN DISCRIMINATOR (2x as much as the Generator)\n",
        "     \n",
        "      for j in range(1, 3):\n",
        "        \n",
        "        z = genNormVector(gan_batch_size, gan_feature_length) # 1. Create your vector z\n",
        "        fake_images = generator(z).to(device).detach()  # 3 Generate your fake image and get loss\n",
        "\n",
        "        dis_optimizer.zero_grad()\n",
        "        d_loss, real_loss, fake_loss = discriminator_loss(gan_batch_size, real_images, fake_images, discriminator, dcounter)\n",
        "        d_loss.backward()\n",
        "        dis_optimizer.step()\n",
        "        \n",
        "        info2 = { 'real_loss': real_loss.item(), 'fake_loss': fake_loss.item() }\n",
        "        for tag, value in info2.items():\n",
        "          logger.scalar_summary(tag, value, dcounter)  \n",
        "        dcounter = dcounter + 1\n",
        "        \n",
        "      # Get the first image and then the image for each subsequent epoch\n",
        "      if ((epoch == 0 and i ==0) or  i == iter_per_epoch - 1):\n",
        "#         show(make_grid(fake_images.detach(), padding=5))\n",
        "        save_image(fake_images, './gans/image{}.png'.format(counter))\n",
        "        counter = counter + 1\n",
        "           \n",
        "      # 1. Log scalar values (scalar summary)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMgl9HAuE2FY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_generator = Generator().to(device)\n",
        "real_discriminator = Discriminator().to(device)\n",
        "\n",
        "gan_training_loop(real_generator, real_discriminator,  3000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p6pnH6Zk9flO",
        "outputId": "cb5e45c9-dd90-44d2-eaa9-b98cb185703b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ngrok already installed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1kECzeR99flT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d4161af3-3c5f-4b27-9c3f-055ff345b0b1",
        "id": "5qeQcPe19flV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorboard Link: https://2482deb4.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4-wwtp_L9gai",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "now = time.mktime(datetime.datetime.now().timetuple())\n",
        "logger = Logger(f'./logs/2cccc_{now}/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9n2FavfCLZoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}